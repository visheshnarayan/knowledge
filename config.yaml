# Configuration for the Knowledge Graph Builder

# Data Ingestion
data:
  input_dir: "data/input"

output:
  base_dir: "data/builds"
  version: "ollama" # or a specific name like "latest", "v1", etc.
  serve_only_mode: false
  run_mode: ['cli'] # Options: 'web', 'cli', or both in a list

# Graph Construction
graph:
  enable_consolidation: false
  chunking:
    strategy: "fixed_size"
    size: 512
    overlap: 50
  embedding:
    model: "bert-base-uncased"
  similarity:
    metric: "cosine"
    threshold: 0.8
  search_similarity: 0.4 # New parameter for agent context search similarity
  search_sample_ratio: 0.8 # Ratio of neighboring nodes to sample for initial search
  triplet_extraction:
    model: "Babelscape/rebel-large"



llm_infra:
  type: 'ollama' # 'strands', 'lm_studio', or 'ollama'
  strands:
    # parent_agent_model_id: "anthropic.claude-3-sonnet-20240229-v1:0"
    parent_agent_model_id: "us.deepseek.r1-v1:0"
    child_agent_model_id: "anthropic.claude-3-sonnet-20240229-v1:0"
    agent:
      system_prompt: "You are a helpful assistant that can answer questions based on the context provided. Please be concise and stick to the information given in the context."
  lm_studio:
    base_url: "http://localhost:1234/v1"
    api_key: "not-needed"
    model: "local-model"
  ollama:
    base_url: "http://localhost:11434/v1"
    api_key: "not-needed"
    model: "llama3:8b"

# Organizational Table settings
org_table:
  load_from_file: true
  path: "data/builds/ollama/org_table.json"

# TODO: replace with pinecone
database:
  uri: "bolt://localhost:7687"
  user: "admin"
  password: "password"

# Debugging settings
debug:
  log_llm_responses: false
